library(plotly)
path_root=".."
path_features=file.path(path_root, "features")
path_models=file.path(path_root, "models")
path_tuned=file.path(path_models, "tuned")
path_random_forest=file.path(path_tuned, "random_forest")
artifacts <- read_rds(file.path(path_features, "feature_engineering_artifacts_list.rds"))
splits <- artifacts$splits
# k = 10 folds
set.seed(123)
resamples_kfold <- training(splits) %>%
vfold_cv(v = 10)
# Registers the doFuture parallel processing
registerDoFuture()
n_cores <- parallel::detectCores()
}
#-------------------------------------------------------------------------
#-------------------------------------------------------------------------
# https://juliasilge.com/blog/sf-trees-random-tuning/
# Connects to prophet::prophet() and xgboost::xgb.train()
model_spec_random_forest_tune <- rand_forest(
mode = "regression",
mtry = tune(),
) %>%
set_engine("ranger")
wflw_spec_random_forest_tune <- workflow() %>%
add_model(model_spec_random_forest_tune) %>%
add_recipe(
artifacts$recipes$recipe_spec %>%
# update_role(Month, new_role = "indicator")
# update_role() doesn't work with Data features, we must replace by step_rm()
step_rm(Month)
)
#-------------------------------------------------------------------------
#-------------------------------------------------------------------------
set.seed(123)
grid_spec1 <- grid_latin_hypercube(
# parameters(model_spec_random_forest_tune) %>%
hardhat::extract_parameter_set_dials(model_spec_random_forest_tune) %>%
update(mtry = mtry(range = c(1, 49))),
size=20
)
# plan(strategy = sequential)
plan( # toggle on parallel processing
strategy = cluster,
workers = parallel::makeCluster(n_cores)
)
# tic()
tuned_results_random_forest1 <- wflw_spec_random_forest_tune %>%
tune_grid(
resamples = resamples_kfold,
grid = grid_spec1,
control = control_grid(verbose = TRUE, allow_par = TRUE)
)
# toc()
tuned_results_random_forest1 %>%
write_rds(file.path(path_random_forest, "tuned_results_random_forest1.rds"))
# toggle off parallel processing
plan(strategy = sequential)
#-------------------------------------------------------------------------
#-------------------------------------------------------------------------
# plan(strategy = sequential)
plan( # toggle on parallel processing
strategy = cluster,
workers = parallel::makeCluster(n_cores)
)
# Fitting round 3 best RMSE model
set.seed(123)
wflw_fit_random_forest_tuned <- wflw_spec_random_forest_tune %>%
finalize_workflow(
select_best(tuned_results_random_forest1, "rmse", n = 1)
) %>%
fit(training(splits))
wflw_fit_random_forest_tuned %>%
write_rds(file.path(path_random_forest, "wflw_fit_random_forest_tuned.rds"))
# toggle off parallel processing
plan(strategy = sequential)
#-------------------------------------------------------------------------
# Fitting round 3 best RSQmodel
# plan(strategy = sequential)
plan( # toggle on parallel processing
strategy = cluster,
workers = parallel::makeCluster(n_cores)
)
set.seed(123)
wflw_fit_random_forest_tuned_rsq <- wflw_spec_random_forest_tune %>%
finalize_workflow(
select_best(tuned_results_random_forest1, "rsq", n = 1)
) %>%
fit(training(splits))
wflw_fit_random_forest_tuned_rsq %>%
write_rds(file.path(path_random_forest, "wflw_fit_random_forest_tuned_rsq.rds"))
# toggle off parallel processing
plan(strategy = sequential)
#-------------------------------------------------------------------------
#-------------------------------------------------------------------------
tuned_random_forest <- list(
# Workflow spec
tuned_wkflw_spec = wflw_spec_random_forest_tune,
# Grid spec
tune_grid_spec = list(
round1 = grid_spec1
),
# Tuning Results
tuned_results = list(
round1 = tuned_results_random_forest1
),
# Tuned Workflow Fit
tuned_wflw_fit = wflw_fit_random_forest_tuned,
# from FE
splits        = artifacts$splits,
data          = artifacts$data,
recipes       = artifacts$recipes,
standardize   = artifacts$standardize,
normalize     = artifacts$normalize
)
tuned_random_forest %>%
write_rds(file.path(path_random_forest, "tuned_random_forest.rds"))
wflw_fit_random_forest_tuned_rsq
rm(list = ls())
rm(list = ls())
# graphics.off()
if (T){
# 1 FEATURE ENGINEERING
library(tidyverse) # loading dplyr, tibble, ggplot2, .. dependencies
library(timetk) # using timetk plotting, diagnostics and augment operations
library(tsibble) # for month to Date conversion
library(tsibbledata) # for aus_retail dataset
library(fastDummies) # for dummyfying categorical variables
library(skimr) # for quick statistics
# 2 FEATURE ENGINEERING WITH RECIPES
library(tidymodels)
# 3 MACHINE LEARNING
library(modeltime)
library(tictoc)
# 4 HYPERPARAMETER TUNING
library(future)
library(doFuture)
library(plotly)
path_root=".."
path_features=file.path(path_root, "features")
path_models=file.path(path_root, "models")
path_tuned=file.path(path_models, "tuned")
path_prophet=file.path(path_tuned, "prophet")
artifacts <- read_rds(file.path(path_features, "feature_engineering_artifacts_list.rds"))
splits <- artifacts$splits
# k = 10 folds
set.seed(123)
resamples_kfold <- training(splits) %>%
vfold_cv(v = 10)
# Registers the doFuture parallel processing
registerDoFuture()
n_cores <- parallel::detectCores()
}
#-------------------------------------------------------------------------
#-------------------------------------------------------------------------
# Connects to prophet::prophet() and xgboost::xgb.train()
model_spec_prophet_tune <- prophet_reg(
mode = "regression",
changepoint_num = tune(),
seasonality_yearly = TRUE,
seasonality_weekly = FALSE,
seasonality_daily = FALSE,
# changepoint_range = tune(),
# prior_scale_changepoints = tune(),
# prior_scale_seasonality = tune(),
# prior_scale_holidays = tune()
) %>%
set_engine("prophet")
wflw_spec_prophet_tune <- workflow() %>%
add_model(model_spec_prophet_tune) %>%
add_recipe(artifacts$recipes$recipe_spec)
#-------------------------------------------------------------------------
#-------------------------------------------------------------------------
set.seed(123)
# changepoint_num(range = c(28,32))
# changepoint_range()
# https://github.com/business-science/modeltime/pull/56
grid_spec1 <- grid_latin_hypercube(
# parameters(model_spec_prophet_tune) %>%
hardhat::extract_parameter_set_dials(model_spec_prophet_tune) %>%
update(mtry = mtry(range = c(1, 50))),
size=20
)
rm(list = ls())
set.seed(123)
grid_spec1 <- grid_latin_hypercube(
parameters(
model_spec_prophet_tune # changepoint_range()
),
size = 20
)
set.seed(123)
grid_spec1 <- grid_latin_hypercube(
parameters(
changepoint_range()
),
size = 20
)
# plan(strategy = sequential)
plan( # toggle on parallel processing
strategy = cluster,
workers = parallel::makeCluster(n_cores)
)
setwd("~/Dropbox/TEACHING/Schools/1_NUS/Courses/Material/2022/Private/1-modeltime/wip")
rm(list = ls())
if (T){
# 1 FEATURE ENGINEERING
library(tidyverse) # loading dplyr, tibble, ggplot2, .. dependencies
library(timetk) # using timetk plotting, diagnostics and augment operations
library(tsibble) # for month to Date conversion
library(tsibbledata) # for aus_retail dataset
library(fastDummies) # for dummyfying categorical variables
library(skimr) # for quick statistics
# 2 FEATURE ENGINEERING WITH RECIPES
library(tidymodels)
# 3 MACHINE LEARNING
library(modeltime)
library(tictoc)
# 4 HYPERPARAMETER TUNING
library(future)
library(doFuture)
library(plotly)
path_root=".."
path_features=file.path(path_root, "features")
path_models=file.path(path_root, "models")
path_tuned=file.path(path_models, "tuned")
path_prophet=file.path(path_tuned, "prophet")
artifacts <- read_rds(file.path(path_features, "feature_engineering_artifacts_list.rds"))
splits <- artifacts$splits
# k = 10 folds
set.seed(123)
resamples_kfold <- training(splits) %>%
vfold_cv(v = 10)
# Registers the doFuture parallel processing
registerDoFuture()
n_cores <- parallel::detectCores()
}
#-------------------------------------------------------------------------
#-------------------------------------------------------------------------
# Connects to prophet::prophet() and xgboost::xgb.train()
model_spec_prophet_tune <- prophet_reg(
mode = "regression",
changepoint_num = tune(),
seasonality_yearly = TRUE,
seasonality_weekly = FALSE,
seasonality_daily = FALSE,
# changepoint_range = tune(),
# prior_scale_changepoints = tune(),
# prior_scale_seasonality = tune(),
# prior_scale_holidays = tune()
) %>%
set_engine("prophet")
wflw_spec_prophet_tune <- workflow() %>%
add_model(model_spec_prophet_tune) %>%
add_recipe(artifacts$recipes$recipe_spec)
set.seed(123)
grid_spec1 <- grid_latin_hypercube(
parameters(
changepoint_range()
),
size = 20
)
# plan(strategy = sequential)
plan( # toggle on parallel processing
strategy = cluster,
workers = parallel::makeCluster(n_cores)
)
tuned_results_prophet1 <- wflw_spec_prophet_tune %>%
tune_grid(
resamples = resamples_kfold,
grid = grid_spec1,
control = control_grid(verbose = TRUE, allow_par = TRUE)
)
set.seed(123)
grid_spec1 <- grid_latin_hypercube(
parameters(
wflw_spec_prophet_tune # changepoint_range()
),
size = 20
)
model_spec_prophet_tune
set.seed(123)
grid_spec1 <- grid_latin_hypercube(
parameters(
hardhat::extract_parameter_set_dials(model_spec_prophet_tune)
# changepoint_range()
),
size = 20
)
set.seed(123)
grid_spec1 <- grid_latin_hypercube(
hardhat::extract_parameter_set_dials(model_spec_prophet_tune),
size = 20
)
# plan(strategy = sequential)
plan( # toggle on parallel processing
strategy = cluster,
workers = parallel::makeCluster(n_cores)
)
tuned_results_prophet1 <- wflw_spec_prophet_tune %>%
tune_grid(
resamples = resamples_kfold,
grid = grid_spec1,
control = control_grid(verbose = TRUE, allow_par = TRUE)
)
tuned_results_prophet1
# toc()
tuned_results_prophet1 %>%
write_rds(file.path(path_prophet, "tuned_results_prophet1.rds"))
# toggle off parallel processing
plan(strategy = sequential)
# plan(strategy = sequential)
plan( # toggle on parallel processing
strategy = cluster,
workers = parallel::makeCluster(n_cores)
)
# Fitting round 3 best RMSE model
set.seed(123)
wflw_fit_prophet_tuned <- wflw_spec_prophet_tune %>%
finalize_workflow(
select_best(tuned_results_prophet1, "rmse", n = 1)
) %>%
fit(training(splits))
wflw_fit_prophet_tuned %>%
write_rds(file.path(path_prophet, "wflw_fit_prophet_tuned.rds"))
# toggle off parallel processing
plan(strategy = sequential)
#-------------------------------------------------------------------------
# Fitting round 3 best RSQmodel
# plan(strategy = sequential)
plan( # toggle on parallel processing
strategy = cluster,
workers = parallel::makeCluster(n_cores)
)
set.seed(123)
wflw_fit_prophet_tuned_rsq <- wflw_spec_prophet_tune %>%
finalize_workflow(
select_best(tuned_results_prophet1, "rsq", n = 1)
) %>%
fit(training(splits))
wflw_fit_prophet_tuned_rsq %>%
write_rds(file.path(path_prophet, "wflw_fit_prophet_tuned_rsq.rds"))
# toggle off parallel processing
plan(strategy = sequential)
wflw_fit_prophet_tuned_rsq
# toggle off parallel processing
plan(strategy = sequential)
#-------------------------------------------------------------------------
#-------------------------------------------------------------------------
tuned_prophet <- list(
# Workflow spec
tuned_wkflw_spec = wflw_spec_prophet_tune,
# Grid spec
tune_grid_spec = list(
round1 = grid_spec1
),
# Tuning Results
tuned_results = list(
round1 = tuned_results_prophet1
),
# Tuned Workflow Fit
tuned_wflw_fit = wflw_fit_prophet_tuned,
# from FE
splits        = artifacts$splits,
data          = artifacts$data,
recipes       = artifacts$recipes,
standardize   = artifacts$standardize,
normalize     = artifacts$normalize
)
tuned_prophet %>%
write_rds(file.path(path_prophet, "tuned_prophet.rds"))
wflw_fit_prophet_tuned
tuned_results_prophet1
tuned_prophet %>%
write_rds(file.path(path_prophet, "tuned_prophet.rds"))
rm(list = ls())
# graphics.off()
if (T){
# 1 FEATURE ENGINEERING
library(tidyverse) # loading dplyr, tibble, ggplot2, .. dependencies
library(timetk) # using timetk plotting, diagnostics and augment operations
library(tsibble) # for month to Date conversion
library(tsibbledata) # for aus_retail dataset
library(fastDummies) # for dummyfying categorical variables
library(skimr) # for quick statistics
# 2 FEATURE ENGINEERING WITH RECIPES
library(tidymodels)
# 3 MACHINE LEARNING
library(modeltime)
library(tictoc)
# 4 HYPERPARAMETER TUNING
library(future)
library(doFuture)
library(plotly)
path_root=".."
path_features=file.path(path_root, "features")
path_models=file.path(path_root, "models")
path_tuned=file.path(path_models, "tuned")
path_random_forest=file.path(path_tuned, "random_forest")
artifacts <- read_rds(file.path(path_features, "feature_engineering_artifacts_list.rds"))
splits <- artifacts$splits
# k = 10 folds
set.seed(123)
resamples_kfold <- training(splits) %>%
vfold_cv(v = 10)
# Registers the doFuture parallel processing
registerDoFuture()
n_cores <- parallel::detectCores()
}
#-------------------------------------------------------------------------
#-------------------------------------------------------------------------
# https://juliasilge.com/blog/sf-trees-random-tuning/
# Connects to prophet::prophet() and xgboost::xgb.train()
model_spec_random_forest_tune <- rand_forest(
mode = "regression",
mtry = tune(),
) %>%
set_engine("ranger")
wflw_spec_random_forest_tune <- workflow() %>%
add_model(model_spec_random_forest_tune) %>%
add_recipe(
artifacts$recipes$recipe_spec %>%
# update_role(Month, new_role = "indicator")
# update_role() doesn't work with Data features, we must replace by step_rm()
step_rm(Month)
)
#-------------------------------------------------------------------------
#-------------------------------------------------------------------------
set.seed(123)
grid_spec1 <- grid_latin_hypercube(
# parameters(model_spec_random_forest_tune) %>%
hardhat::extract_parameter_set_dials(model_spec_random_forest_tune) %>%
update(mtry = mtry(range = c(1, 49))),
size=20
)
# plan(strategy = sequential)
plan( # toggle on parallel processing
strategy = cluster,
workers = parallel::makeCluster(n_cores)
)
# tic()
tuned_results_random_forest1 <- wflw_spec_random_forest_tune %>%
tune_grid(
resamples = resamples_kfold,
grid = grid_spec1,
control = control_grid(verbose = TRUE, allow_par = TRUE)
)
# toc()
tuned_results_random_forest1 %>%
write_rds(file.path(path_random_forest, "tuned_results_random_forest1.rds"))
# toggle off parallel processing
plan(strategy = sequential)
#-------------------------------------------------------------------------
#-------------------------------------------------------------------------
# plan(strategy = sequential)
plan( # toggle on parallel processing
strategy = cluster,
workers = parallel::makeCluster(n_cores)
)
# Fitting round 3 best RMSE model
set.seed(123)
wflw_fit_random_forest_tuned <- wflw_spec_random_forest_tune %>%
finalize_workflow(
select_best(tuned_results_random_forest1, "rmse", n = 1)
) %>%
fit(training(splits))
wflw_fit_random_forest_tuned %>%
write_rds(file.path(path_random_forest, "wflw_fit_random_forest_tuned.rds"))
# toggle off parallel processing
plan(strategy = sequential)
#-------------------------------------------------------------------------
# Fitting round 3 best RSQmodel
# plan(strategy = sequential)
plan( # toggle on parallel processing
strategy = cluster,
workers = parallel::makeCluster(n_cores)
)
set.seed(123)
wflw_fit_random_forest_tuned_rsq <- wflw_spec_random_forest_tune %>%
finalize_workflow(
select_best(tuned_results_random_forest1, "rsq", n = 1)
) %>%
fit(training(splits))
wflw_fit_random_forest_tuned_rsq %>%
write_rds(file.path(path_random_forest, "wflw_fit_random_forest_tuned_rsq.rds"))
# toggle off parallel processing
plan(strategy = sequential)
#-------------------------------------------------------------------------
#-------------------------------------------------------------------------
tuned_random_forest <- list(
# Workflow spec
tuned_wkflw_spec = wflw_spec_random_forest_tune,
# Grid spec
tune_grid_spec = list(
round1 = grid_spec1
),
# Tuning Results
tuned_results = list(
round1 = tuned_results_random_forest1
),
# Tuned Workflow Fit
tuned_wflw_fit = wflw_fit_random_forest_tuned,
# from FE
splits        = artifacts$splits,
data          = artifacts$data,
recipes       = artifacts$recipes,
standardize   = artifacts$standardize,
normalize     = artifacts$normalize
)
tuned_random_forest %>%
write_rds(file.path(path_random_forest, "tuned_random_forest.rds"))
